\chapter{Matching} \label{chapter:matching}
In order to analyze the lead-lag relationship of corporate bonds and stocks, we need a large, survivorship bias free database with both stock and bond returns for any given point in time. So far, we only have two separate databases -- one with historical corporate bonds data, and one with historical equity data. Therefore, the two databases have to be joined into one, based on the company that issued both. The task is not as trivial as it might seem, since there is no unique company identifier available in both databases. In the following, the available matching options will be discussed, and the most suitable approach chosen. 

\section{Available Options} \label{section:available-options}
To begin with, the following extracted bond and equity parameters were considered for the matching: 
\begin{itemize}
	\item SEDOL code
	\item WKN code
	\item CUSIP-9 code
	\item ISIN code
	\item Worldscope identifier
	\item Company name
\end{itemize}

\subsection{SEDOL}
The SEDOL is a unique 7-character identification code which stands for 'Stock Exchange Daily Official List' \cite{sedol}. It is issued for securities registered in the United Kingdom and Ireland by the London Stock Exchange. Despite being used to uniquely identify securities, it does not, in general, contain a unique issuing company identifier, because the codes are simply issued sequentially. For example, two bonds, which were both issued by Apple Inc., can have the SEDOL codes \textit{BF43J24} and \textit{BK9WPP6}, respectively. The only similarity between the two is that these were issued only two years apart, and thus have the \textit{B} at the beginning in common. Besides, the identifier is not available in our stocks database, and only exists for securities of companies listed on the LSE. 

\subsection{WKN}
The WKN is a German 6-digit alphanumeric security identification code and stands for 'Wertpapierkennnummer'. Since 2004, it is possible for companies to obtain a WKN with a unique company identifier included \cite{wkn}. A WKN includes a company identifier if it starts with at least two characters before proceeding with digits. However, not all companies make use of this opportunity when ordering a WKN for their securities. Taking into account that there are also multiple exceptions from the rule base of WKN identifiers, it is hard to use these as unique company identifiers. This is especially the case because WKN are generally only available for German securities. Also, the parameter is not available in our existing equities database. 

\subsection{CUSIP-9}
The CUSIP number is a unique identification number assigned to all equities and bonds that are registered in the United States and Canada \cite{cusip}. The CUSIP consists of 9 alphanumeric characters, of which the first 6 comprise the unique issuing company identifier. The code is often used in one of its shorter forms, i.e. as CUSIP-8 and CUSIP-6. However, in our case, only the CUSIP-6 variant is of interest. It can be derived from CUSIP-9 by simply dropping the last three characters. The CUSIP-9 code is directly available in our equities database. In the bonds database, it can only be found directly for some of the securities in the so-called \textit{local code} variable (LOC), which can be found in Datastream. Unfortunately, the CUSIP values entered in this variable are not very reliable. A workaround can be achieved by using the security ISIN, as will be explained in \ref{section:cusip-matching}. 

\subsection{ISIN}
The ISIN stands for 'International Securities Identification Number' and is an international standard way to uniquely identify securities \cite{isin}. The ISIN by itself is not a unique company identifier. However, it sometimes contains a company identifier as part of it. In particular, for U.S. and Canadian securities, the ISIN usually contains the Cusip-9 code, which, in its turn, contains a 6-digit unique company identifier. For U.K. and Irish securities, the ISIN usually contains the SEDOL code. And for German securities, the ISIN contains the WKN. Therefore, while the ISIN itself cannot be directly used for the matching, it can nevertheless be used to obtain missing matching code values by extracting them from the ISIN. 

\subsection{Worldscope identifier}
The Worldscope Identifier is a 9-digit code issued by Worldscope, a Thomson Reuters' fundamentals product \cite{worldscope}. It is used to uniquely identify both issuing companies and securities. For U.S. companies, the Worldscope Identifier is identical with the CUSIP-9 code. For non-U.S. companies, a derived identifier is used, based on the country where the issuing company is domiciled, and also includes a unique company code. A more detailed explanation of the mechanics can be found in the Datastream database or the appendix to this work. %TODO ref appendix screenshots
Unfortunately, the Worldscope Identifier is only available in the equities database, and not for bonds. Therefore, it cannot be used for the matching. 

\subsection{Company name}
As the 'method of last resort', the company name itself can be used to join the bond and equity databases. The problem with company names as identifiers is though that these are not necessarily unique on the one hand, and also tend to have heterogeneous spelling -- i.e. one and the same company can be spelled in multiple different ways across the database. To provide an example, the company names \textit{THE WILLIAMS COMPANIES INCO} and \textit{WILLIAMS PARTNERS L.P.} refer to the same company, but are written differently, which makes the join between the two databases ambiguous. Nevertheless, the approach can be a good starting point when other options are not available, and will be introduced in greater detail in the next section. 

\section{Fuzzy String Matching} \label{section:fuzzy-string-matching}
Having considered the different options to perform the matching, it becomes apparent that the only unique identifier which can be reliably used for the task is the CUSIP-9 code. Additionally, the company name can be used to produce a solid baseline to start with. In this section, a matching approach called Fuzzy String Matching will be introduced as a means to join the datasets via company name. In the next section, a matching approach based on the CUSIP code will be explained. 

The term Fuzzy String Matching \cite{fuzzy-1, fuzzy-2} -- also called Approximate String Matching -- refers to use cases when there are two or more strings which have the same meaning, but are spelled somewhat differently. There exist multiple approaches to measure the extent of 'difference' between strings. The formula which is most commonly used is the so-called Levenshtein distance \cite{levenshtein}. It measures the minimum number of single-character edits required to change one given sequence into the other. An \textit{edit}, in its turn, is defined as one of the three operations performed on a string: 
\begin{itemize}
	\item insertion
	\item deletion
	\item substitution
\end{itemize}
Depending on the implementation, a substitution of a character can count as either one or two edits. This is because a substitution technically consists of both an insertion and a deletion. For simplicity reasons it can be assumed to count as one edit just like the other two operations. To give an example, consider a company that is called \textit{Apple Inc.} in one dataset and \textit{Apple Incorp.} in the other. The Levenshtein distance between the two company names would be 3, because exactly 3 insertions need to be performed in order to produce \textit{Apple Incorp.} from \textit{Apple Inc.} These insertions are the 3 characters \textit{o}, \textit{r} and \textit{p}. Alternatively, we can also start the other way around, from the company name \textit{Apple Incorp.} In this case, we would need 3 deletions to produce \textit{Apple Inc.} In particular, we would have to delete the 3 characters \textit{o}, \textit{r} and \textit{p}. 

While there exists a concrete formula which defines the Levenshtein distance, it is not very relevant in this context, since we will not be implementing the Levenshtein distance ourselves. Instead, we will make use of dedicated Python packages, which compute the Levenshtein distance between two strings behind the scenes in order to produce a similarity score. In this work, we consider the two packages \textit{fuzzywuzzy}\footnote{https://pypi.org/project/fuzzywuzzy} and \textit{rapidfuzz}\footnote{https://pypi.org/project/rapidfuzz} for the task. In reality, these packages do slightly more than just computing the edit distance, depending on the particular function called. A detailed description of these packages' capabilities can be found in their respective documentation. 

The concept will be used in our case to select for each company name from the bonds database the one from the equities database which has the smallest Levenshtein distance to it. This way, matching company names from the two datasets will be connected to each other to produce a join. 

\subsection{Fuzzy-Wuzzy}
\textit{Fuzzywuzzy} is the most commonly used package for fuzzy string matching in the Python community. Therefore, my first approach to perform the matching was with the fuzzywuzzy package. It requires additionally the package \textit{python-Levenshtein} to be installed, so it can use its faster C implementation of the Levenshtein distance. The rest of the \textit{fuzzywuzzy} package is programmed in Python. 

To start with the implementation, the static bond and equity data needs to be read into \textit{pandas}\footnote{https://pandas.pydata.org} dataframes to perform further operations on it. For this purpose, it is advisable to export the static data from Stata to the CSV\footnote{CSV means comma-separated-values and is a frequently used data storage format. In the first row of a CSV file there are usually column headers, all separated by commas. In all further rows the single column values are stored, also separated by commas. The format is frequently used in data science applications due it being lightweight and information-dense. } format, and then to import it in Python from CSV files. I explicitly discourage exporting data from Excel to CSV, because Excel has its own understanding of the CSV format, which might not be compatible with \textit{pandas}. 

After the data has been loaded into dataframes, one for bond company names, and one for stock company names, it can be fed into one of the predefined \textit{fuzzywuzzy} functions. To start simple, two company names can be compared to each other with the built-in function ratio(), which computes the standard Levenshtein distance similarity ratio between the two sequences. Note that this function also takes into account whether the characters are capitalized or not. Thus, \textit{Apple Inc. }and \textit{apple Inc.} would produce a similarity ratio lower than 100\% due to the difference in the capitalization of the first letter. This is a not very desired behavior for our use case, because we do not care much whether a company's name is written in upper or lower case letters. There exist modifications to this function, such as partial\_ratio(), which can also detect similarities within substrings, or token\_sort\_ratio, which can detect similarities between substrings which are differently positioned. The function which is best-fitted to our use case is extractOne(). For each bond company name, it evaluates the Levenshtein similarity score with all stock company names. The one with the highest similarity score is considered a match. 

In practice, this approach can be implemented with two nested for-loops, which is not very efficient, but necessary, because we have no index structure on our datasets. The resulting algorithm is thus somewhat similar to a standard Nested-Loop-Join. Faster approaches for unstructured data like a Sort-Merge-Join or a Hash-Join would not work, since the company names are not unique. For the resulting matching, it suffices to store the \textit{dscd} pairs of the bonds and equities which were determined to be most likely join partners. By the \textit{dscd} codes, any other static or time series data can be joined in later, because the Datastream code is unique security identifier and present in all tuples from both static and time series databases. Keep in mind that if the Datastream code is called \textit{dscd} for both stocks and bonds, you will first have to rename it in e.g. \textit{bond\_dscd} and \textit{stock\_dscd }first to avoid ambiguity. The results of the matching can be exported from a \textit{pandas} dataframe to CSV format again. This CSV file can then be imported e.g. into Stata for further processing. 

Despite its convenience of rapid prototyping, the \textit{fuzzywuzzy} package has turned out to be rather slow in practice. Based on time measurements for 1,000 bonds, it would need around 150 hours of computation time to complete the matching, if scaled up to all the bonds in our database. On the one hand, it is not surprising, since we have around 60,000 bonds and 80,000 equities in our respective datasets, and are running a nested loop join of a sort on them. On the other hand, a faster approach would be preferred to avoid long waiting times. A better approach to this task is provided by the \textit{rapidfuzz} package. 

\subsection{Rapidfuzz}
\textit{Rapidfuzz} is a (rather unknown) Python package which takes \textit{fuzzywuzzy} as a base, and improves it by not only implementing the Levenshtein distance in C, but also the rest of the package in C++. This and also some algorithmic improvements make it significantly faster than original \textit{fuzzywuzzy}. It has a somewhat more complex interface, but provides a very noticeable boost to the matching program. The general approach is very similar to matching with \textit{fuzzywuzzy}: The static data is imported to pandas dataframes in CSV format and gets joined with some of the functions the package provides. Since \textit{rapidfuzz} is based on \textit{fuzzywuzzy}, the author has kept some of the function definitions similar to the original package. Therefore, extractOne() is still the best-fitting matching function for our use case, because it returns the most similar matching partner to the given bond company name. 

To additionally improve the matching quality, another optimization should be performed on both bond and stock company names before the matching is done. To reduce spelling differences \`{a}-priori, all company names should be cast to lower case, and all punctuation signs and special symbols should be removed beforehand. This will make sure that when the data is fed into the matching algorithm, it will only need to compare the similarity based on the semantics of the sequences, without taking into account 'unnecessary' symbols. The resulting performance is significantly improved compared to the \textit{fuzzywuzzy} approach. The updated matching algorithm only needs around 7 hours to compute the matching, compared to the 150 hours from before, which is a running time reduction of 95\%. 

Just like before, the results should be stored in dataframe format first, and then exported to CSV for future needs. It is sensible to not only export \textit{dscd} pairs, but also the similarity score between the two. This makes it possible to cut off insufficiently matched data for certain analyses depending on the needs. I set the original score cut-off at runtime to 90\% similarity to avoid having many false positives. More on this in section \ref{section:matching-evaluation}. The entire matching program code can be found in the file \textit{matching.py} as part of this project. 

\section{CUSIP Matching} \label{section:cusip-matching}
Having accomplished a baseline matching with the Fuzzy String Matching approach, it can be further refined by using the CUSIP-9 identifier. As mentioned in section \ref{section:available-options}, the identifier is already available in the equities database. However, in the bonds dataset, it is only given as part of the \textit{local code} (LOC), where it is not reliably entered, and thus cannot be used for matching. At this point, we can make use of the fact that the ISIN of U.S. and Canadian securities includes the CUSIP-9 identifier as part of it. This means that in order to obtain the CUSIP code for the bonds we simply have to take the alphanumeric characters 3 to 11 of the ISIN. Taking into account that we are actually interested in the CUSIP-6 instead of CUSIP-9, because only its first 6 digits represent a unique company identifier, it is enough to take the characters 3 to 8 of the ISIN. 

As there is no complex data science involved here, the procedure can be done directly in Stata. At first, the static bond and equity data needs to be loaded into Stata, if not already done so during data preparation as explained in section \ref{chapter:data-preparation}. Since the CUSIP identifier can only be used to match U.S. and Canadian securities, both bonds and equities need to be filtered by these two countries. The rest of the countries can be dropped from the dataset for this purpose. Remember to always work on a local copy of the original dataset so as to not irreversibly lose data. The next step is to generate new variables for the CUSIP-6 identifier. For equities, this can be done with the Stata command \lstinline|gen cusip_6 = substr(cusip_9, 1, 6)|. For bonds -- with \lstinline|gen cusip_6 = substr(ISIN, 3, 6)|. Finally, the two datasets need to be merged. This can be done with the \lstinline|merge| command on a N:M relation. 
Herewith, the matching over the CUSIP-9 identifier is accomplished and the results can be saved. 

\section{Evaluation} \label{section:matching-evaluation}
The results of the two matching approaches are as following: 
\begin{itemize}
	\item With Fuzzy String Matching, around 26,032 bonds have found an equity matching partner. This equals appx. 42.89\% of the total 60,688 bonds. 
	\item With the CUSIP-9 approach, 6,460 (North American) bonds have found an equity match. This equals appx. 10.64\% of the total 60,688 bonds. 
\end{itemize}
While the Fuzzy String Matching approach has a significantly higher matching ratio, one should keep in mind that the CUSIP-9 results are more reliable, because it is a unique key attribute and not a fuzzy one. For further analysis, it makes sense to merge the two matching tables to come up with one single matching database of highest possible accuracy. For this, we can use the \textit{append} command from Stata to simply concatenate the two datasets. Remember to make sure that both datasets have the same variables and same variable names in order for the union to work. For example, one might need to create an empty similarity\_score variable in the CUSIP matching first, because it is present in the fuzzy string matching. 
As soon as the matching tables have been concatenated, the resulting dataset will contain duplicates in respect to bond\_dscd and stock\_dscd parameters. You can check this by running the command \lstinline|duplicates report bond_dscd stock_dscd|. This is due to the two matchings having overlaps. While the duplicates should be removed, it is important to keep the CUSIP matching pair and not the fuzzy matching one for each encountered duplicate. This is because for the CUSIP matching we can be 100\% sure that it is accurate. The information that a bond and an equity are perfect matches, and not just based on a similarity score of e.g. 92\%, might be useful in later analyses. 
After the duplicates have been removed, the total matching ratio increases from 42.89\% (26,032 bonds with fuzzy matching only) to 44.91\% (28,254 bonds with both approaches combined). While the improvement is only minor, keep in mind that some of the matching pairs are now more reliable than with fuzzy string matching alone. It is hard to give an concise measure of how many matches are perfect matches without manually checking all of them. But since the similarity score was already over 90\% for the fuzzy string matching, my estimation is that around 40\% of the pairs are perfect matches. 







